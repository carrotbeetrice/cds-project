{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np, pandas as pd, pickle, time, argparse\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx\n",
    "\n",
    "def read_glove_vecs(glove_file: str) -> tuple:\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            current_word = line[0]\n",
    "            words.add(current_word)\n",
    "            word_to_vec_map[current_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        i = 1\n",
    "        words_to_index, index_to_words = {}, {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i += 1\n",
    "\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def preprocess_text(text:str)->str:\n",
    "    text = text.lower()\n",
    "    text = nfx.remove_punctuations(text)\n",
    "    text = nfx.remove_stopwords(text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "def create_utterances(filename:str, split: str):\n",
    "    sentences, act_labels, emotion_labels, speakers, conv_id, utt_id = [], [], [], [], [], []\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for c_id, line in enumerate(f):\n",
    "            s = eval(line)\n",
    "            for u_id, item in enumerate(s['dialogue']):\n",
    "                sentences.append(item['text'])\n",
    "                act_labels.append(item['act'])\n",
    "                emotion_labels.append(item['emotion'])\n",
    "                conv_id.append(split[:2] + \"_c\" + str(c_id))\n",
    "                utt_id.append(split[:2] + \"_c\" + str(c_id) + \"_u\" + str(u_id))\n",
    "                speakers.append(str(u_id%2))\n",
    "\n",
    "    data = pd.DataFrame(sentences, columns=['sentence'])\n",
    "    data['sentence'] = data['sentence'].apply(lambda x: preprocess_text(x))\n",
    "    data['act_label'] = act_labels\n",
    "    data['emotion_label'] = emotion_labels\n",
    "    data['speaker'] = speakers\n",
    "    data['conv_id'] = conv_id\n",
    "    data['utt_id'] = utt_id\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets\n",
    "DAILYDIALOG_DIR_PATH = \"../data/dailydialog/\"\n",
    "\n",
    "train_data = create_utterances(DAILYDIALOG_DIR_PATH + 'train.json', 'train')\n",
    "valid_data = create_utterances(DAILYDIALOG_DIR_PATH + 'valid.json', 'valid')\n",
    "test_data = create_utterances(DAILYDIALOG_DIR_PATH + 'test.json', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode emotion and dialog act labels\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "all_act_labels, all_emotion_labels = set(train_data[\"act_label\"]), set(train_data[\"emotion_label\"])\n",
    "act_label_encoder, emotion_label_encoder, act_label_decoder, emotion_label_decoder = {}, {}, {}, {}\n",
    "\n",
    "for i, label in enumerate(all_act_labels):\n",
    "    act_label_encoder[label] = i\n",
    "    act_label_decoder[i] = label\n",
    "\n",
    "for i, label in enumerate(all_emotion_labels):\n",
    "    emotion_label_encoder[label] = i\n",
    "    emotion_label_decoder[i] = {}\n",
    "\n",
    "train_data['encoded_act_label'] = train_data['act_label'].map(lambda x: encode_labels(act_label_encoder, x))\n",
    "test_data['encoded_act_label'] = test_data['act_label'].map(lambda x: encode_labels(act_label_encoder, x))\n",
    "valid_data['encoded_act_label'] = valid_data['act_label'].map(lambda x: encode_labels(act_label_encoder, x))\n",
    "\n",
    "train_data['encoded_emotion_label'] = train_data['emotion_label'].map(lambda x: encode_labels(emotion_label_encoder, x))\n",
    "test_data['encoded_emotion_label'] = test_data['emotion_label'].map(lambda x: encode_labels(emotion_label_encoder, x))\n",
    "valid_data['encoded_emotion_label'] = valid_data['emotion_label'].map(lambda x: encode_labels(emotion_label_encoder, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all sentences\n",
    "all_text = list(train_data['sentence'])\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences into sequences\n",
    "train_sequence = tokenizer.texts_to_sequences(list(train_data['sentence']))\n",
    "valid_sequence = tokenizer.texts_to_sequences(list(valid_data['sentence']))\n",
    "test_sequence = tokenizer.texts_to_sequences(list(test_data['sentence']))\n",
    "\n",
    "train_data['sentence_length'] = [len(item) for item in train_sequence]\n",
    "valid_data['sentence_length'] = [len(item) for item in valid_sequence]\n",
    "test_data['sentence_length'] = [len(item) for item in test_sequence]\n",
    "\n",
    "MAX_NUM_TOKENS = 250\n",
    "\n",
    "train_sequence = pad_sequences(train_sequence, maxlen=MAX_NUM_TOKENS, padding='post')\n",
    "valid_sequence = pad_sequences(valid_sequence, maxlen=MAX_NUM_TOKENS, padding='post')\n",
    "test_sequence = pad_sequences(test_sequence, maxlen=MAX_NUM_TOKENS, padding='post')\n",
    "\n",
    "train_data['sequence'] = list(train_sequence)\n",
    "valid_data['sequence'] = list(valid_sequence)\n",
    "test_data['sequence'] = list(test_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network with Pre-trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, vocab_size, ouput_dim, batch_size) -> None:\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = embedding\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # LSTM takes word embeddings as inputs and outputs hidden states with dimensionality hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, dropout=0.5, batch_first=True)\n",
    "\n",
    "        # Fully-connected linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, ouput_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "\n",
    "        # Initialise hidden state\n",
    "        h0 = torch.zeros(2, sentence.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(2, sentence.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        lstm_out, h = self.lstm(embeds, (h0, c0))\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # Dropout\n",
    "        lstm_out = F.dropout(lstm_out, 0.5)\n",
    "\n",
    "        fc_out = self.fc(lstm_out)\n",
    "\n",
    "        out = F.softmax(fc_out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GloVe Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map: dict, word_to_index: dict, non_trainable: bool=True) -> tuple:\n",
    "    num_embeddings = len(word_to_index) + 1\n",
    "    random_word = sample(word_to_vec_map.keys(), 1)[0]\n",
    "    embedding_dim = word_to_vec_map[random_word].shape[0]\n",
    "    \n",
    "    # Initialise embedding matrix as a numpy array of zeroes of shape num_embeddings * embedding_dim\n",
    "    weights_matrix = np.zeros((num_embeddings, embedding_dim))\n",
    "\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation \n",
    "    # of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        weights_matrix[index, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embed = nn.Embedding.from_pretrained(torch.from_numpy(weights_matrix).type(torch.FloatTensor), freeze=non_trainable)\n",
    "\n",
    "    return embed, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequences\n",
    "X_train = train_data['sequence'] #.to_numpy()\n",
    "X_test = test_data['sequence'] #.to_numpy()\n",
    "X_valid = valid_data['sequence'] #.to_numpy()\n",
    "\n",
    "y_train = train_data['encoded_emotion_label'].to_numpy()\n",
    "y_test = test_data['encoded_emotion_label'].to_numpy()\n",
    "y_valid = valid_data['encoded_emotion_label'].to_numpy()\n",
    "\n",
    "# Convert emotion labels to one-hot vectors\n",
    "C = train_data['encoded_emotion_label'].value_counts().size\n",
    "y_oh_train = convert_to_one_hot(y_train, C)\n",
    "y_oh_test = convert_to_one_hot(y_test, C)\n",
    "y_oh_valid = convert_to_one_hot(y_valid, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../glove_model/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-07bd32ff251b>:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random_word = sample(word_to_vec_map.keys(), 1)[0]\n"
     ]
    }
   ],
   "source": [
    "embedding, vocab_size, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 128\n",
    "OUTPUT_SIZE = 7\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-fa55b7cc28fd>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  train_dataset = TensorDataset(torch.Tensor(X_train).type(torch.LongTensor), torch.tensor(y_train).type(torch.LongTensor))\n"
     ]
    }
   ],
   "source": [
    "# Generate dataloaders\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train).type(torch.LongTensor), torch.tensor(y_train).type(torch.LongTensor))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test).type(torch.LongTensor), torch.tensor(y_test).type(torch.LongTensor))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TensorDataset(torch.Tensor(X_valid).type(torch.LongTensor), torch.tensor(y_valid).type(torch.LongTensor))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
    "    model.to(device)\n",
    "    running_loss = 0\n",
    "    train_losses, test_losses, accuracies = [], [], []\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for sentences, labels in tqdm(trainloader, f\"EPOCH {e + 1}\"):\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() # Erase previous gradients\n",
    "            pred = model.forward(sentences) # Make a prediction\n",
    "            loss = criterion(pred, labels) # Calculate how much we missed\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() # Log our progress\n",
    "\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            # Turn off gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for sentences, labels in test_loader:\n",
    "                    sentences, labels = sentences.to(device), labels.to(device)\n",
    "                    log_ps = model(sentences)\n",
    "                    test_loss += criterion(log_ps, labels)\n",
    "\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "            test_losses.append(test_loss / len(test_loader))\n",
    "            accuracies.append(accuracy / len(test_loader) * 100)\n",
    "\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "                \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "                \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "\n",
    "            \n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(train_losses, c='b', label='Training loss')\n",
    "    plt.plot(test_losses, c='r', label='Testing loss')\n",
    "    plt.xticks(np.arange(0, epochs))\n",
    "    plt.title('Losses')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(accuracies)\n",
    "    plt.xticks(np.arange(0, epochs))\n",
    "    plt.title('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def train(dataloader):\n",
    "#     model.train()\n",
    "#     # total_acc, total_count = 0, 0\n",
    "#     # log_interval = 50\n",
    "#     # start_time = time.time()\n",
    "\n",
    "#     for sentences, labels in tqdm(dataloader, \"Training in progress...\"):\n",
    "#         sentences, labels = sentences.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad() # Erase previous gradients\n",
    "#         pred = model.forward(sentences) # Make a prediction\n",
    "#         loss = criterion(pred, labels) # Calculate how much we missed\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # log_ps = model(sentences)\n",
    "#         # # loss = criterion(log_ps, labels)\n",
    "\n",
    "#         # ps = torch.exp(log_ps)\n",
    "#         # top_p, top_class = ps.topk(1, dim=1)\n",
    "#         # equals = top_class == labels.view(*top_class.shape)\n",
    "#         # total_acc += torch.mean(equals.type(torch.FloatTensor))\n",
    "#         # total_count += label.size(0)\n",
    "#         # if idx % log_interval == 0 and idx > 0:\n",
    "#         #     elapsed = time.time() - start_time\n",
    "#         #     print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "#         #           '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "#         #                                       total_acc/total_count))\n",
    "#         #     total_acc, total_count = 0, 0\n",
    "#         #     start_time = time.time()\n",
    "\n",
    "# def evaluate(dataloader):\n",
    "#     model.eval()\n",
    "#     total_acc, total_count = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for sentences, labels in tqdm(dataloader, \"Evaluating model...\"):\n",
    "#             # pred = model(sentences)\n",
    "#             # loss = criterion(pred, label)\n",
    "#             # total_acc += (pred.argmax(1) == label).sum().item()\n",
    "#             # total_count += label.size(0)\n",
    "#             log_ps = model(sentences)\n",
    "#             # loss = criterion(log_ps, labels)\n",
    "\n",
    "#             ps = torch.exp(log_ps)\n",
    "#             top_p, top_class = ps.topk(1, dim=1)\n",
    "#             equals = top_class == labels.view(*top_class.shape)\n",
    "#             total_acc += torch.mean(equals.type(torch.FloatTensor))\n",
    "#             total_count += labels.size(0)\n",
    "\n",
    "#     return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 1: 100%|██████████| 2725/2725 [19:18<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3..  Training Loss: 1.340..  Test Loss: 1.349..  Test Accuracy: 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 2: 100%|██████████| 2725/2725 [22:34<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3..  Training Loss: 1.338..  Test Loss: 1.349..  Test Accuracy: 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 3: 100%|██████████| 2725/2725 [14:49<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3..  Training Loss: 1.338..  Test Loss: 1.349..  Test Accuracy: 0.817\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training time!\n",
    "model = Net(embedding, embedding_dim, HIDDEN_DIM, vocab_size, OUTPUT_SIZE, BATCH_SIZE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "total_accu = None\n",
    "train(model, train_loader, criterion, optimizer, EPOCHS)\n",
    "# N = 10\n",
    "# acc = 0\n",
    "# epoch_counter = 0\n",
    "\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train(train_loader)\n",
    "    \n",
    "#     # Question 5: Use your validation set to early stop the model. Remember to early stop when the validation accuracy does not improve for continous N number of epochs where N is a hyperparameter. Set N = 10\n",
    "#     val_acc = evaluate(valid_loader)\n",
    "#     if (acc >= val_acc):\n",
    "#         epoch_counter += 1\n",
    "#         if (epoch_counter == N): \n",
    "#             print(\"Validation set accuracy = \" + str(val_acc) + \"%\")\n",
    "#             break\n",
    "#     else:\n",
    "#         acc = val_acc\n",
    "#         epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.349..  Test Accuracy: 0.817\n"
     ]
    }
   ],
   "source": [
    "# Test model accuracy\n",
    "\n",
    "test_loss = 0\n",
    "accuracy = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sentences, labels in test_loader:\n",
    "        sentences, labels = sentences.to(device), labels.to(device)\n",
    "        ps = model(sentences)\n",
    "        test_loss += criterion(ps, labels).item()\n",
    "\n",
    "        # Accuracy\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "model.train()\n",
    "print(\"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "      \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
